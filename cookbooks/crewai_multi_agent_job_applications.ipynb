{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7d655c1-c81d-4e79-abdf-fc363ee83a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3 crewai crewai_tools langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40457a16-8a69-4c69-892f-1c04e6b59862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ab7b853-ed3d-4155-80f8-08e910dda6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "boto_session = boto3.Session(profile_name=\"sandbox\", region_name=\"us-west-2\")\n",
    "bedrock_runtime=boto_session.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "}\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "858f76cf-b9d2-4ceb-b37b-c547324561f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Serper API key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"SERPER_API_KEY\"] = getpass.getpass(\"Serper API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6a505e-855c-4596-b701-e5507db3cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aa1caf7-cac1-4dec-9dab-68312c07eb8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaError",
     "evalue": "Key 'embedder' error:\nKey 'provider' error:\nOr('openai', 'gpt4all', 'huggingface', 'vertexai', 'azure_openai', 'google', 'mistralai', 'nvidia') did not validate 'aws_bedrock'\n'openai' does not match 'aws_bedrock'\n'gpt4all' does not match 'aws_bedrock'\n'huggingface' does not match 'aws_bedrock'\n'vertexai' does not match 'aws_bedrock'\n'azure_openai' does not match 'aws_bedrock'\n'google' does not match 'aws_bedrock'\n'mistralai' does not match 'aws_bedrock'\n'nvidia' does not match 'aws_bedrock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/schema.py:542\u001b[0m, in \u001b[0;36mSchema.validate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SchemaError \u001b[38;5;28;01mas\u001b[39;00m x:\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/schema.py:219\u001b[0m, in \u001b[0;36mOr.validate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         errors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _x\u001b[38;5;241m.\u001b[39merrors\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m SchemaError(\n\u001b[1;32m    220\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m did not validate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m, data)] \u001b[38;5;241m+\u001b[39m autos,\n\u001b[1;32m    221\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error\u001b[38;5;241m.\u001b[39mformat(data) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m errors,\n\u001b[1;32m    222\u001b[0m )\n",
      "\u001b[0;31mSchemaError\u001b[0m: Or('openai', 'gpt4all', 'huggingface', 'vertexai', 'azure_openai', 'google', 'mistralai', 'nvidia') did not validate 'aws_bedrock'\n'openai' does not match 'aws_bedrock'\n'gpt4all' does not match 'aws_bedrock'\n'huggingface' does not match 'aws_bedrock'\n'vertexai' does not match 'aws_bedrock'\n'azure_openai' does not match 'aws_bedrock'\n'google' does not match 'aws_bedrock'\n'mistralai' does not match 'aws_bedrock'\n'nvidia' does not match 'aws_bedrock'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/schema.py:483\u001b[0m, in \u001b[0;36mSchema.validate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m     nvalue \u001b[38;5;241m=\u001b[39m \u001b[43mSchema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43msvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_extra_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SchemaError \u001b[38;5;28;01mas\u001b[39;00m x:\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/schema.py:544\u001b[0m, in \u001b[0;36mSchema.validate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SchemaError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SchemaError(\n\u001b[1;32m    545\u001b[0m         [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mautos, [e\u001b[38;5;241m.\u001b[39mformat(data) \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39merrors\n\u001b[1;32m    546\u001b[0m     )\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m x:\n",
      "\u001b[0;31mSchemaError\u001b[0m: Or('openai', 'gpt4all', 'huggingface', 'vertexai', 'azure_openai', 'google', 'mistralai', 'nvidia') did not validate 'aws_bedrock'\n'openai' does not match 'aws_bedrock'\n'gpt4all' does not match 'aws_bedrock'\n'huggingface' does not match 'aws_bedrock'\n'vertexai' does not match 'aws_bedrock'\n'azure_openai' does not match 'aws_bedrock'\n'google' does not match 'aws_bedrock'\n'mistralai' does not match 'aws_bedrock'\n'nvidia' does not match 'aws_bedrock'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/schema.py:483\u001b[0m, in \u001b[0;36mSchema.validate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m     nvalue \u001b[38;5;241m=\u001b[39m \u001b[43mSchema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43msvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_extra_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SchemaError \u001b[38;5;28;01mas\u001b[39;00m x:\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/schema.py:489\u001b[0m, in \u001b[0;36mSchema.validate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepend_schema_name(k)\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SchemaError(\n\u001b[1;32m    490\u001b[0m         [message] \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mautos,\n\u001b[1;32m    491\u001b[0m         [e\u001b[38;5;241m.\u001b[39mformat(data) \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39merrors,\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSchemaError\u001b[0m: Key 'provider' error:\nOr('openai', 'gpt4all', 'huggingface', 'vertexai', 'azure_openai', 'google', 'mistralai', 'nvidia') did not validate 'aws_bedrock'\n'openai' does not match 'aws_bedrock'\n'gpt4all' does not match 'aws_bedrock'\n'huggingface' does not match 'aws_bedrock'\n'vertexai' does not match 'aws_bedrock'\n'azure_openai' does not match 'aws_bedrock'\n'google' does not match 'aws_bedrock'\n'mistralai' does not match 'aws_bedrock'\n'nvidia' does not match 'aws_bedrock'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m scrape_tool \u001b[38;5;241m=\u001b[39m ScrapeWebsiteTool()\n\u001b[1;32m     10\u001b[0m read_resume \u001b[38;5;241m=\u001b[39m FileReadTool(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./sample/fake_resume.md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m semantic_search_resume \u001b[38;5;241m=\u001b[39m \u001b[43mMDXSearchTool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmdx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./sample/fake_resume.md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maws_bedrock\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manthropic.claude-3-sonnet-20240229-v1:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maws_bedrock\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamazon.titan-embed-text-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m           \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/crewai_tools/tools/mdx_seach_tool/mdx_search_tool.py:32\u001b[0m, in \u001b[0;36mMDXSearchTool.__init__\u001b[0;34m(self, mdx, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mdx: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mdx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(mdx)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/crewai_tools/tools/rag/rag_tool.py:47\u001b[0m, in \u001b[0;36mRagTool._set_default_adapter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01membedchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m App\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madapters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedchain_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbedchainAdapter\n\u001b[0;32m---> 47\u001b[0m     app \u001b[38;5;241m=\u001b[39m \u001b[43mApp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;28;01melse\u001b[39;00m App()\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;241m=\u001b[39m EmbedchainAdapter(\n\u001b[1;32m     49\u001b[0m         embedchain_app\u001b[38;5;241m=\u001b[39mapp, summarize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummarize\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/embedchain/app.py:363\u001b[0m, in \u001b[0;36mApp.from_config\u001b[0;34m(cls, config_path, config, auto_deploy, yaml_path)\u001b[0m\n\u001b[1;32m    360\u001b[0m     config_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Validate the config\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[43mvalidate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m app_config_data \u001b[38;5;241m=\u001b[39m config_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    366\u001b[0m vector_db_config_data \u001b[38;5;241m=\u001b[39m config_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectordb\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/embedchain/utils/misc.py:503\u001b[0m, in \u001b[0;36mvalidate_config\u001b[0;34m(config_data)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_config\u001b[39m(config_data):\n\u001b[1;32m    383\u001b[0m     schema \u001b[38;5;241m=\u001b[39m Schema(\n\u001b[1;32m    384\u001b[0m         {\n\u001b[1;32m    385\u001b[0m             Optional(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp\u001b[39m\u001b[38;5;124m\"\u001b[39m): {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m         }\n\u001b[1;32m    501\u001b[0m     )\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/LLM_Cookbooks/venv/lib/python3.11/site-packages/schema.py:489\u001b[0m, in \u001b[0;36mSchema.validate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m error:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m nkey\n\u001b[1;32m    488\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepend_schema_name(k)\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SchemaError(\n\u001b[1;32m    490\u001b[0m         [message] \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mautos,\n\u001b[1;32m    491\u001b[0m         [e\u001b[38;5;241m.\u001b[39mformat(data) \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39merrors,\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     new[nkey] \u001b[38;5;241m=\u001b[39m nvalue\n",
      "\u001b[0;31mSchemaError\u001b[0m: Key 'embedder' error:\nKey 'provider' error:\nOr('openai', 'gpt4all', 'huggingface', 'vertexai', 'azure_openai', 'google', 'mistralai', 'nvidia') did not validate 'aws_bedrock'\n'openai' does not match 'aws_bedrock'\n'gpt4all' does not match 'aws_bedrock'\n'huggingface' does not match 'aws_bedrock'\n'vertexai' does not match 'aws_bedrock'\n'azure_openai' does not match 'aws_bedrock'\n'google' does not match 'aws_bedrock'\n'mistralai' does not match 'aws_bedrock'\n'nvidia' does not match 'aws_bedrock'"
     ]
    }
   ],
   "source": [
    "from crewai_tools import (\n",
    "  FileReadTool,\n",
    "  ScrapeWebsiteTool,\n",
    "  MDXSearchTool,\n",
    "  SerperDevTool\n",
    ")\n",
    "\n",
    "search_tool = SerperDevTool()\n",
    "scrape_tool = ScrapeWebsiteTool()\n",
    "read_resume = FileReadTool(file_path='./sample/fake_resume.md')\n",
    "semantic_search_resume = MDXSearchTool(\n",
    "    mdx='./sample/fake_resume.md',\n",
    "    config=dict(\n",
    "        llm=dict(\n",
    "            provider=\"aws_bedrock\",\n",
    "            config=dict(\n",
    "                model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                model_kwargs =  { \n",
    "                    \"max_tokens\": 512,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"top_k\": 250,\n",
    "                    \"top_p\": 1,\n",
    "                }\n",
    "                \n",
    "            ),\n",
    "        ),\n",
    "        embedder=dict(\n",
    "            provider=\"aws_bedrock\",\n",
    "            config=dict(\n",
    "                model=\"amazon.titan-embed-text-v1\",\n",
    "                model_kwargs =  { \n",
    "                    \"max_tokens\": 512,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"top_k\": 250,\n",
    "                    \"top_p\": 1,\n",
    "                }           \n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17388139-cc6e-4bae-aeca-795ae5695523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "# display(Markdown(\"./fake_resume.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee04dd0-cdd6-47ca-a041-5c65a929704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: Researcher\n",
    "researcher = Agent(\n",
    "    role=\"Tech Job Researcher\",\n",
    "    goal=\"Make sure to do amazing analysis on \"\n",
    "         \"job posting to help job applicants\",\n",
    "    llm=llm,\n",
    "    tools = [scrape_tool, search_tool],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"As a Job Researcher, your prowess in \"\n",
    "        \"navigating and extracting critical \"\n",
    "        \"information from job postings is unmatched.\"\n",
    "        \"Your skills help pinpoint the necessary \"\n",
    "        \"qualifications and skills sought \"\n",
    "        \"by employers, forming the foundation for \"\n",
    "        \"effective application tailoring.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca18c53-9e53-40a5-b92d-a268d27c4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 2: Profiler\n",
    "profiler = Agent(\n",
    "    role=\"Personal Profiler for Engineers\",\n",
    "    goal=\"Do increditble research on job applicants \"\n",
    "         \"to help them stand out in the job market\",\n",
    "    llm=llm,\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"Equipped with analytical prowess, you dissect \"\n",
    "        \"and synthesize information \"\n",
    "        \"from diverse sources to craft comprehensive \"\n",
    "        \"personal and professional profiles, laying the \"\n",
    "        \"groundwork for personalized resume enhancements.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1ddf6-aa5e-4f8a-8ef2-396bc9125cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 3: Resume Strategist\n",
    "resume_strategist = Agent(\n",
    "    role=\"Resume Strategist for Engineers\",\n",
    "    goal=\"Find all the best ways to make a \"\n",
    "         \"resume stand out in the job market.\",\n",
    "    llm=llm,\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"With a strategic mind and an eye for detail, you \"\n",
    "        \"excel at refining resumes to highlight the most \"\n",
    "        \"relevant skills and experiences, ensuring they \"\n",
    "        \"resonate perfectly with the job's requirements.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee35ae6-297e-4dd5-825c-7d60eb03135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 4: Interview Preparer\n",
    "interview_preparer = Agent(\n",
    "    role=\"Engineering Interview Preparer\",\n",
    "    goal=\"Create interview questions and talking points \"\n",
    "         \"based on the resume and job requirements\",\n",
    "    llm=llm,\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"Your role is crucial in anticipating the dynamics of \"\n",
    "        \"interviews. With your ability to formulate key questions \"\n",
    "        \"and talking points, you prepare candidates for success, \"\n",
    "        \"ensuring they can confidently address all aspects of the \"\n",
    "        \"job they are applying for.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454170e-9660-4813-b299-17df222fee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task for Researcher Agent: Extract Job Requirements\n",
    "research_task = Task(\n",
    "    description=(\n",
    "        \"Analyze the job posting URL provided ({job_posting_url}) \"\n",
    "        \"to extract key skills, experiences, and qualifications \"\n",
    "        \"required. Use the tools to gather content and identify \"\n",
    "        \"and categorize the requirements.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A structured list of job requirements, including necessary \"\n",
    "        \"skills, qualifications, and experiences.\"\n",
    "    ),\n",
    "    agent=researcher,\n",
    "    async_execution=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f872b-f710-4f16-822e-ff35735a066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task for Profiler Agent: Compile Comprehensive Profile\n",
    "profile_task = Task(\n",
    "    description=(\n",
    "        \"Compile a detailed personal and professional profile \"\n",
    "        \"using the GitHub ({github_url}) URLs, and personal write-up \"\n",
    "        \"({personal_writeup}). Utilize tools to extract and \"\n",
    "        \"synthesize information from these sources.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A comprehensive profile document that includes skills, \"\n",
    "        \"project experiences, contributions, interests, and \"\n",
    "        \"communication style.\"\n",
    "    ),\n",
    "    agent=profiler,\n",
    "    async_execution=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a25f78-2c40-42db-acf3-eb7318853865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task for Resume Strategist Agent: Align Resume with Job Requirements\n",
    "resume_strategy_task = Task(\n",
    "    description=(\n",
    "        \"Using the profile and job requirements obtained from \"\n",
    "        \"previous tasks, tailor the resume to highlight the most \"\n",
    "        \"relevant areas. Employ tools to adjust and enhance the \"\n",
    "        \"resume content. Make sure this is the best resume even but \"\n",
    "        \"don't make up any information. Update every section, \"\n",
    "        \"inlcuding the initial summary, work experience, skills, \"\n",
    "        \"and education. All to better reflrect the candidates \"\n",
    "        \"abilities and how it matches the job posting.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"An updated resume that effectively highlights the candidate's \"\n",
    "        \"qualifications and experiences relevant to the job.\"\n",
    "    ),\n",
    "    output_file=\"tailored_resume.md\",\n",
    "    context=[research_task, profile_task],\n",
    "    agent=resume_strategist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bba786-1905-45a2-b46b-37fdd76863cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task for Interview Preparer Agent: Develop Interview Materials\n",
    "interview_preparation_task = Task(\n",
    "    description=(\n",
    "        \"Create a set of potential interview questions and talking \"\n",
    "        \"points based on the tailored resume and job requirements. \"\n",
    "        \"Utilize tools to generate relevant questions and discussion \"\n",
    "        \"points. Make sure to use these question and talking points to \"\n",
    "        \"help the candiadte highlight the main points of the resume \"\n",
    "        \"and how it matches the job posting.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A document containing key questions and talking points \"\n",
    "        \"that the candidate should prepare for the initial interview.\"\n",
    "    ),\n",
    "    output_file=\"./sample/interview_materials.md\",\n",
    "    context=[research_task, profile_task, resume_strategy_task],\n",
    "    agent=interview_preparer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84d360-827a-4775-8ac1-fdea8f1306b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_application_crew = Crew(\n",
    "    agents=[researcher,\n",
    "            profiler,\n",
    "            resume_strategist,\n",
    "            interview_preparer],\n",
    "\n",
    "    tasks=[research_task,\n",
    "           profile_task,\n",
    "           resume_strategy_task,\n",
    "           interview_preparation_task],\n",
    "\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce23faf-77a5-4436-9cda-2e7f1d500734",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_application_inputs = {\n",
    "    'job_posting_url': 'https://jobs.lever.co/AIFund/6c82e23e-d954-4dd8-a734-c0c2c5ee00f1?lever-origin=applied&lever-source%5B%5D=AI+Fund',\n",
    "    'github_url': 'https://github.com/joaomdmoura',\n",
    "    'personal_writeup': \"\"\"Noah is an accomplished Software\n",
    "    Engineering Leader with 18 years of experience, specializing in\n",
    "    managing remote and in-office teams, and expert in multiple\n",
    "    programming languages and frameworks. He holds an MBA and a strong\n",
    "    background in AI and data science. Noah has successfully led\n",
    "    major tech initiatives and startups, proving his ability to drive\n",
    "    innovation and growth in the tech industry. Ideal for leadership\n",
    "    roles that require a strategic and innovative approach.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36782427-62d0-42aa-b300-af5c4a62c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this execution will take a few minutes to run\n",
    "result = job_application_crew.kickoff(inputs=job_application_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa90da2-d5cd-4884-9753-b7dff22484c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"./tailored_resume.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d9ed8-7690-4e75-a93b-0f518490941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"./interview_materials.md\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
